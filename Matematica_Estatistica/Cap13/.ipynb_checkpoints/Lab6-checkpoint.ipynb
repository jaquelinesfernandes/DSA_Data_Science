{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "# <font color='blue'>Data Science Academy</font>\n",
    "## <font color='blue'>Matemática e Estatística Aplicada Para Data Science, Machine Learning e IA</font>\n",
    "## <font color='blue'>Lab 6</font>\n",
    "### <font color='blue'>Usando Cálculo e Limite de Funções em Data Science com Linguagem Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import sympy\n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a Derivada com Linguagem Python\n",
    "\n",
    "A derivada mede a sensibilidade à mudança da função (valor de saída) em relação a uma mudança na sua entrada.\n",
    "\n",
    "Em termos mais simples, a derivada de uma função em um ponto específico é a taxa na qual a função está mudando naquele ponto. Isso é frequentemente entendido como a inclinação da linha tangente à função naquele ponto.\n",
    "\n",
    "Por exemplo, se temos uma função que descreve a posição de um carro em movimento ao longo do tempo, a derivada dessa função em um ponto específico nos dá a velocidade do carro naquele momento.\n",
    "\n",
    "A derivada de uma função f(x) é normalmente escrita como f'(x) ou df/dx. O processo de encontrar a derivada é chamado de diferenciação.\n",
    "\n",
    "Se tivermos uma função f(x) = x^n, onde n é um número real, então a derivada desta função é dada por f'(x) = n * x^(n-1).\n",
    "\n",
    "Há regras especiais (como a regra do produto, regra do quociente e a regra da cadeia) para lidar com funções mais complexas.\n",
    "\n",
    "Para calcular a derivada de uma função em Python, você pode usar o módulo sympy, que é uma biblioteca Python para matemática simbólica.\n",
    "\n",
    "Vamos calcular a derivada da função f(x) = x^3 + 2x^2 - x + 1 no ponto x = 1.\n",
    "\n",
    "Primeiro, você precisará importar o módulo sympy e definir as variáveis simbólicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variável simbólica x\n",
    "x = symbols('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função\n",
    "f = x**3 + 2*x**2 - x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a derivada de f\n",
    "derivada_f = diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Derivada da função: {derivada_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso vai imprimir a derivada da função, que é 3x^2 + 4x - 1.\n",
    "\n",
    "Agora, para avaliar a derivada no ponto x = 1, você pode substituir x por 1 na derivada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia a derivada no ponto x = 1\n",
    "valor = derivada_f.subs(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Valor da derivada no ponto x=1: {valor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que a derivada da função no ponto x = 1 é 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variável simbólica x\n",
    "x = symbols('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função\n",
    "f = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a derivada de f\n",
    "derivada_f = diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Derivada da função: {derivada_f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia a derivada no ponto x = 5\n",
    "valor = derivada_f.subs(x, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Valor da derivada no ponto x=5: {valor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação Geométrica da Derivada em Python\n",
    "\n",
    "- Passo 1: Escolher uma Função\n",
    "\n",
    "Primeiro, escolheremos uma função para a qual queremos calcular a derivada. Por exemplo, vamos usar a função quadrática f(x)=x^2.\n",
    "\n",
    "- Passo 2: Plotar a Função\n",
    "\n",
    "Depois criamos valores randômicos e plotamos a função.\n",
    "\n",
    "- Passo 3: Escolher um Ponto e Calcular a Inclinação da Secante\n",
    "\n",
    "Escolheremos um ponto na função e calcularemos a inclinação da linha secante, que passa por esse ponto e outro ponto muito próximo a ele. Essa inclinação é uma aproximação da derivada.\n",
    "\n",
    "- Passo 4: Diminuir a Distância entre os Pontos\n",
    "\n",
    "Diminuiremos gradualmente a distância entre os dois pontos e observaremos como a inclinação da linha secante se aproxima da inclinação da linha tangente.\n",
    "\n",
    "- Passo 5: Calcular a Derivada\n",
    "\n",
    "Finalmente, mostraremos como a inclinação da linha secante converge para a inclinação da linha tangente, que é a derivada da função no ponto escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passos 1 e 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Passo 1: Definindo a função\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Passo 2: Plotando a função\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, label=\"f(x) = x²\")\n",
    "plt.title(\"Gráfico de f(x) = x²\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos escolher um ponto na curva, por exemplo, x=5, e calcular a inclinação da linha secante entre x=5 e outro ponto próximo, digamos x=6. A inclinação da linha secante é dada pela diferença das funções dividida pela diferença dos x's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 3: Escolher um ponto e calcular a inclinação da secante\n",
    "\n",
    "# Ponto escolhido e ponto próximo\n",
    "x1, x2 = 5, 6\n",
    "\n",
    "# Calculando a inclinação da secante\n",
    "secant_slope = (f(x2) - f(x1)) / (x2 - x1)\n",
    "\n",
    "# Criando a linha secante\n",
    "secant_line = secant_slope * (x - x1) + f(x1)\n",
    "\n",
    "# Plotando a função e a linha secante\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, label=\"f(x) = x²\")\n",
    "plt.plot(x, secant_line, label=\"Linha Secante em x = 5\", linestyle='--')\n",
    "plt.scatter([x1, x2], [f(x1), f(x2)], color='red') # Pontos na curva\n",
    "plt.title(\"Linha Secante Aproximando a Derivada\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "secant_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos diminuir a distância entre os dois pontos para ver como a inclinação da linha secante se aproxima da inclinação da linha tangente (derivada) em x=5. Vamos fazer isso para x = 5.1, 5.01, 5.001 e observar as mudanças.\n",
    "\n",
    "Para cada novo valor de x, recalcularemos a inclinação da secante e plotaremos as linhas secantes para visualizar a convergência para a linha tangente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passos 4 e 5: Diminuindo a distância entre os pontos e visualizando a convergência\n",
    "\n",
    "# Novos pontos próximos\n",
    "x2_values = [5.1, 5.01, 5.001]\n",
    "\n",
    "# Plotando a função\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, label=\"f(x) = x²\")\n",
    "\n",
    "# Calculando e plotando as novas linhas secantes\n",
    "for x2 in x2_values:\n",
    "    secant_slope = (f(x2) - f(x1)) / (x2 - x1)\n",
    "    secant_line = secant_slope * (x - x1) + f(x1)\n",
    "    plt.plot(x, secant_line, label=f\"Linha Secante em x2 = {x2}\", linestyle='--')\n",
    "\n",
    "# Adicionando o ponto de tangência\n",
    "plt.scatter([x1], [f(x1)], color='red') # Ponto na curva\n",
    "plt.title(\"Convergência das Linhas Secantes para a Derivada\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico está mostrando como as linhas secantes se aproximam da linha tangente à medida que a distância entre os pontos diminui. Como você pode ver, à medida que x se aproxima de 5, a inclinação da secante (a linha pontilhada) se aproxima da inclinação da linha tangente no ponto x=5.\n",
    "\n",
    "Este processo ilustra como a derivada, que é a inclinação da linha tangente, pode ser aproximada por linhas secantes. À medida que a distância entre os pontos na linha secante se torna infinitesimalmente pequena, a inclinação da secante converge para a inclinação da tangente, que é a derivada da função no ponto dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função Composta - Regra da Cadeia (Chain Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Regra da Cadeia (ou Chain Rule, em inglês) é uma fórmula para calcular a derivada de uma composição de funções. Em outras palavras, é usada quando temos uma \"função dentro de uma função\", também conhecida como função composta.\n",
    "\n",
    "Vamos considerar duas funções, f(x) e g(x). Se temos uma função h(x) que é a composição dessas duas funções, isto é, h(x) = f(g(x)), então a Regra da Cadeia diz que a derivada de h(x) é a derivada de f em relação a g, multiplicada pela derivada de g em relação a x.\n",
    "\n",
    "Matematicamente, isso é expresso da seguinte maneira:\n",
    "\n",
    "h'(x) = f'(g(x)) * g'(x)\n",
    "\n",
    "Essa fórmula nos diz que, para derivar a função composta h(x) = f(g(x)), primeiro derivamos a função externa f em relação à função interna g, e então multiplicamos pelo resultado da derivação da função interna g em relação a x.\n",
    "\n",
    "Vamos ilustrar isso com um exemplo:\n",
    "\n",
    "Suponha que temos h(x) = (3x + 1)^2. Esta é uma composição de f(u) = u^2 e g(x) = 3x + 1. Se quisermos encontrar h'(x), primeiro derivamos f(u) em relação a u para obter 2u, e então substituímos u por g(x) para obter 2*(3x + 1). Depois derivamos g(x) em relação a x para obter 3. \n",
    "\n",
    "Finalmente, multiplicamos esses dois resultados para obter h'(x) = 2 * (3x + 1) * 3 = 6 * (3x + 1) = 18x + 6.\n",
    "\n",
    "A Regra da Cadeia é uma ferramenta fundamental no cálculo diferencial e é frequentemente usada quando se lida com funções compostas.\n",
    "\n",
    "https://www.deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar a biblioteca sympy para calcular a derivada de uma função composta. Vamos considerar a função h(x) = (3x + 1)^2 como mencionado na explicação acima.\n",
    "\n",
    "Aqui está o código Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a variável simbólica x\n",
    "x = symbols('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função composta h(x)\n",
    "h = (3*x + 1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a derivada de h(x) usando a regra da cadeia\n",
    "h_prime = diff(h, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Derivada de (3x + 1)^2: {h_prime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é a derivada de h(x) = (3x + 1)^2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regra da Cadeia em Redes Neurais Artificiais\n",
    "\n",
    "O uso mais comum da regra da cadeia em redes neurais artificiais está na implementação do algoritmo de retropropagação (backpropagation), que é usado para treinar redes neurais.\n",
    "\n",
    "A retropropagação é um algoritmo que calcula o gradiente da função de perda (loss function) com respeito aos pesos da rede. Esse gradiente é então usado para ajustar os pesos na direção que minimiza a perda. A regra da cadeia é usada para calcular esse gradiente.\n",
    "\n",
    "O gradiente de uma função é um vetor que contém as derivadas parciais da função em relação a cada uma de suas variáveis. Ele fornece a direção do maior aumento da função e a magnitude desse aumento é dada pelo valor do gradiente naquele ponto.\n",
    "\n",
    "A regra da cadeia é um teorema no cálculo que permite a diferenciação de funções compostas. No contexto de funções de múltiplas variáveis, a regra da cadeia permite calcular a derivada de uma função composta considerando as derivadas das funções componentes.\n",
    "\n",
    "Quando se calcula o gradiente de uma função composta usando a regra da cadeia, o que se obtém é uma expressão para a taxa de variação da função composta em relação a cada uma de suas variáveis. Em outras palavras, o gradiente resultante nos dá a direção e magnitude do maior aumento da função composta no espaço de várias dimensões.\n",
    "\n",
    "Essa informação é extremamente útil em uma série de aplicações, incluindo otimização de funções, onde se deseja encontrar o ponto mínimo ou máximo de uma função, bem como em métodos numéricos e aplicações de Machine Learning, como no treinamento de redes neurais com o método do gradiente descendente.\n",
    "\n",
    "Vamos implementar uma rede neural simples com apenas um neurônio (também chamado de perceptron) para demonstrar isso. Usaremos a biblioteca PyTorch, que lida automaticamente com a regra da cadeia durante a retropropagação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Considere Fórmula Matemática: y = x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar nosso exemplo, vamos definir os conceitos abaixo:\n",
    "\n",
    "**Cálculo do Gradiente**: Em redes neurais, o processo de aprendizado envolve otimizar os parâmetros (ou pesos) da rede para minimizar a função de perda (ou erro). Isso é feito usando técnicas de otimização como o gradiente descendente. Para aplicar essas técnicas, é necessário calcular o gradiente da função de perda em relação a cada parâmetro. O gradiente é essencialmente a taxa de mudança da função de perda com respeito a esses parâmetros, ou seja, a derivada.\n",
    "\n",
    "**Backpropagation**: O cálculo do gradiente é realizado através de um processo chamado backpropagation. Para isso, as bibliotecas de aprendizado de máquina mantêm um grafo de computação que registra todas as operações realizadas nos tensores que têm requires_grad definido como True. Quando a função de perda é calculada, o gradiente dessa perda é propagado de volta através do grafo e os gradientes em relação a cada tensor são acumulados.\n",
    "\n",
    "**requires_grad=True**: Ao definir requires_grad=True para um tensor no PyTorch, você está informando à biblioteca que deseja que ela calcule os gradientes desse tensor durante a passagem para trás (backpropagation). Normalmente, isso é feito para os parâmetros da rede que você deseja otimizar. Por exemplo, pesos e vieses em uma rede neural teriam requires_grad=True.\n",
    "\n",
    "**Otimização e Atualização de Parâmetros**: Durante o treinamento, esses gradientes são usados por otimizadores (como SGD, Adam, etc.) para atualizar os parâmetros da rede na direção que minimiza a função de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o tensor de entrada x (dado de entrada)\n",
    "x = torch.tensor([10.0], requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o tensor de peso w (coeficiente do modelo, exatamente o que modelo vai aprender no treinamento)\n",
    "w = torch.tensor([0.03], requires_grad = True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o tensor de saída y (aquilo que queremos prever)\n",
    "y = torch.tensor([1.0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função de ativação como a função identidade (f(x) = x)\n",
    "funcao_ativacao = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das principais razões para usar funções de ativação é introduzir não linearidade no modelo. Redes neurais são projetadas para aproximar funções complexas e a maioria dos problemas do mundo real que queremos modelar são não lineares por natureza. Sem funções de ativação não lineares, uma rede neural, independentemente de sua profundidade (número de camadas), seria equivalente a um modelo linear e, portanto, incapaz de modelar a complexidade encontrada em tarefas reais como reconhecimento de imagem, processamento de linguagem natural, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a saída da rede neural\n",
    "y_previsto = funcao_ativacao(w * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_previsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função de perda como o quadrado da diferença entre \n",
    "# a saída do modelo (valor previsto) e o alvo (valor real)\n",
    "funcao_de_erro = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a perda comparando a saída ao alvo\n",
    "erro = funcao_de_erro(y_previsto, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa retropropagação para calcular os gradientes\n",
    "erro.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print do gradiente de w\n",
    "print(f\"Gradiente de w: {w.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O script Python acima define um neurônio com uma entrada (x) e um peso (w), e usa a regra da cadeia para calcular o gradiente da função de perda com respeito ao peso. O valor de gradiente resultante pode ser usado para atualizar o peso e treinar a rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Descendente via Operações Matemáticas com Linguagem Python\n",
    "\n",
    "Uma aplicação comum de derivadas em Data Science é na implementação do algoritmo de gradiente descendente, usado para otimizar funções de custo em modelos de aprendizado de máquina, como regressão linear ou redes neurais.\n",
    "\n",
    "Aqui está um exemplo de como isso pode ser feito em Python para a tarefa de regressão linear. Para simplificar, vamos considerar uma regressão linear simples com apenas uma variável de entrada.\n",
    "\n",
    "No exemplo abaixo, o código cria um conjunto de dados aleatório, inicializa os parâmetros da regressão linear de forma aleatória (a variável theta), e então executa o algoritmo de gradiente descendente por um número fixo de iterações.\n",
    "\n",
    "A cada iteração, o algoritmo calcula o gradiente da função de custo em relação aos parâmetros (gradientes) e, em seguida, atualiza os parâmetros na direção oposta ao gradiente (isso é o que a linha theta = theta - lr * gradientes faz). O tamanho do passo é determinado pela taxa de aprendizado (lr).\n",
    "\n",
    "Ao final do processo, os parâmetros da regressão linear (ou seja, a inclinação e o intercepto) são armazenados na variável theta. Esses parâmetros minimizam a função de custo e, portanto, representam a melhor linha de ajuste aos dados.\n",
    "\n",
    "Considere que X é o diâmetro de uma Pizza que um cliente pediu e y a gorjeta dada por um cliente. Conseguimos prever a gorjeta com base no diâmetro da Pizza?\n",
    "\n",
    "Vamos definir uma relação linear entre X e y:\n",
    "\n",
    "y = coef1 + (coef2 * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos um conjunto de dados sintéticos\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxa de aprendizado\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização dos hiperparâmetros\n",
    "n_iterations = 1000\n",
    "m = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização dos parâmetros (coeficientes)\n",
    "theta = np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona coluna de 1s a X\n",
    "X_b = np.c_[np.ones((m, 1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treino\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    # Calcula o gradiente (derivada parcial)\n",
    "    gradientes = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    \n",
    "    # Usa os gradientes para atualizar os coeficientes\n",
    "    theta = theta - lr * gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = coef1 + (coef2 * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qual a previsão de gorjeta se o cliente pedir uma Pizza de 14 centímetros de diâmetro?\n",
    "y = theta[0] + (theta[1] * 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

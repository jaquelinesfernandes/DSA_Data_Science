{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Curso Bônus - Data Science e Machine Learning com Linguagem Julia</font>\n",
    "\n",
    "## <font color='blue'>Machine Learning com Linguagem Julia</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/MP-MLJulia.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Dropbox/DSA/DSMLJulia/MLJulia/env`\n"
     ]
    }
   ],
   "source": [
    "# Cria e instancia um env\n",
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Dropbox/DSA/DSMLJulia/MLJulia/env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Dropbox/DSA/DSMLJulia/MLJulia/env/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote MLJ\n",
    "Pkg.add(\"MLJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa o pacote\n",
    "using MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\textbf{Author}: R.A. Fisher   \\textbf{Source}: \\href{https://archive.ics.uci.edu/ml/datasets/Iris}{UCI} - 1936 - Donated by Michael Marshall   \\textbf{Please cite}:   \n",
       "\n",
       "\\textbf{Iris Plants Database}   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda \\& Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "\\subsubsection{Attribute Information:}\n",
       "\\begin{verbatim}\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n",
       "\n",
       "**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "### Attribute Information:\n",
       "\n",
       "```\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "```\n"
      ],
      "text/plain": [
       "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n",
       "  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n",
       "  Marshall \u001b[1mPlease cite\u001b[22m:\n",
       "\n",
       "  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n",
       "  the pattern recognition literature. Fisher's paper is a classic in the field\n",
       "  and is referenced frequently to this day. (See Duda & Hart, for example.)\n",
       "  The data set contains 3 classes of 50 instances each, where each class\n",
       "  refers to a type of iris plant. One class is linearly separable from the\n",
       "  other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "  Predicted attribute: class of iris plant. This is an exceedingly simple\n",
       "  domain.\n",
       "\n",
       "\u001b[1m  Attribute Information:\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[36m  1. sepal length in cm\u001b[39m\n",
       "\u001b[36m  2. sepal width in cm\u001b[39m\n",
       "\u001b[36m  3. petal length in cm\u001b[39m\n",
       "\u001b[36m  4. petal width in cm\u001b[39m\n",
       "\u001b[36m  5. class: \u001b[39m\n",
       "\u001b[36m     -- Iris Setosa\u001b[39m\n",
       "\u001b[36m     -- Iris Versicolour\u001b[39m\n",
       "\u001b[36m     -- Iris Virginica\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve o dataset 61\n",
    "OpenML.describe_dataset(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tables.DictColumnTable with 150 rows, 5 columns, and schema:\n",
       " :sepallength  Float64\n",
       " :sepalwidth   Float64\n",
       " :petallength  Float64\n",
       " :petalwidth   Float64\n",
       " :class        CategoricalArrays.CategoricalValue{String, UInt32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o dataset 61\n",
    "iris = OpenML.load(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>150 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>7</th><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>8</th><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>9</th><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>10</th><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>11</th><td>5.4</td><td>3.7</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>12</th><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>13</th><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>14</th><td>4.3</td><td>3.0</td><td>1.1</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>15</th><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>16</th><td>5.7</td><td>4.4</td><td>1.5</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>17</th><td>5.4</td><td>3.9</td><td>1.3</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>18</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>19</th><td>5.7</td><td>3.8</td><td>1.7</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>20</th><td>5.1</td><td>3.8</td><td>1.5</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>21</th><td>5.4</td><td>3.4</td><td>1.7</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>22</th><td>5.1</td><td>3.7</td><td>1.5</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>23</th><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>24</th><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>Iris-setosa</td></tr><tr><th>25</th><td>4.8</td><td>3.4</td><td>1.9</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>26</th><td>5.0</td><td>3.0</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>27</th><td>5.0</td><td>3.4</td><td>1.6</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>28</th><td>5.2</td><td>3.5</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>29</th><td>5.2</td><td>3.4</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>30</th><td>4.7</td><td>3.2</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa \\\\\n",
       "\t7 & 4.6 & 3.4 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t8 & 5.0 & 3.4 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t9 & 4.4 & 2.9 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t10 & 4.9 & 3.1 & 1.5 & 0.1 & Iris-setosa \\\\\n",
       "\t11 & 5.4 & 3.7 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t12 & 4.8 & 3.4 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t13 & 4.8 & 3.0 & 1.4 & 0.1 & Iris-setosa \\\\\n",
       "\t14 & 4.3 & 3.0 & 1.1 & 0.1 & Iris-setosa \\\\\n",
       "\t15 & 5.8 & 4.0 & 1.2 & 0.2 & Iris-setosa \\\\\n",
       "\t16 & 5.7 & 4.4 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t17 & 5.4 & 3.9 & 1.3 & 0.4 & Iris-setosa \\\\\n",
       "\t18 & 5.1 & 3.5 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t19 & 5.7 & 3.8 & 1.7 & 0.3 & Iris-setosa \\\\\n",
       "\t20 & 5.1 & 3.8 & 1.5 & 0.3 & Iris-setosa \\\\\n",
       "\t21 & 5.4 & 3.4 & 1.7 & 0.2 & Iris-setosa \\\\\n",
       "\t22 & 5.1 & 3.7 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t23 & 4.6 & 3.6 & 1.0 & 0.2 & Iris-setosa \\\\\n",
       "\t24 & 5.1 & 3.3 & 1.7 & 0.5 & Iris-setosa \\\\\n",
       "\t25 & 4.8 & 3.4 & 1.9 & 0.2 & Iris-setosa \\\\\n",
       "\t26 & 5.0 & 3.0 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t27 & 5.0 & 3.4 & 1.6 & 0.4 & Iris-setosa \\\\\n",
       "\t28 & 5.2 & 3.5 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t29 & 5.2 & 3.4 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t30 & 4.7 & 3.2 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m150×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class          \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…           \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa\n",
       "   5 │         5.0         3.6          1.4         0.2  Iris-setosa\n",
       "   6 │         5.4         3.9          1.7         0.4  Iris-setosa\n",
       "   7 │         4.6         3.4          1.4         0.3  Iris-setosa\n",
       "   8 │         5.0         3.4          1.5         0.2  Iris-setosa\n",
       "   9 │         4.4         2.9          1.4         0.2  Iris-setosa\n",
       "  10 │         4.9         3.1          1.5         0.1  Iris-setosa\n",
       "  11 │         5.4         3.7          1.5         0.2  Iris-setosa\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮             ⋮\n",
       " 141 │         6.7         3.1          5.6         2.4  Iris-virginica\n",
       " 142 │         6.9         3.1          5.1         2.3  Iris-virginica\n",
       " 143 │         5.8         2.7          5.1         1.9  Iris-virginica\n",
       " 144 │         6.8         3.2          5.9         2.3  Iris-virginica\n",
       " 145 │         6.7         3.3          5.7         2.5  Iris-virginica\n",
       " 146 │         6.7         3.0          5.2         2.3  Iris-virginica\n",
       " 147 │         6.3         2.5          5.0         1.9  Iris-virginica\n",
       " 148 │         6.5         3.0          5.2         2.0  Iris-virginica\n",
       " 149 │         6.2         3.4          5.4         2.3  Iris-virginica\n",
       " 150 │         5.9         3.0          5.1         1.8  Iris-virginica\n",
       "\u001b[36m                                                        129 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converte o dataset em dataframe\n",
    "df = DataFrames.DataFrame(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza as 4 primeiras linhas\n",
    "first(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬──────────────────────────────────┐\n",
       "│\u001b[22m names       \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                            \u001b[0m│\n",
       "├─────────────┼───────────────┼──────────────────────────────────┤\n",
       "│ sepallength │ Continuous    │ Float64                          │\n",
       "│ sepalwidth  │ Continuous    │ Float64                          │\n",
       "│ petallength │ Continuous    │ Float64                          │\n",
       "│ petalwidth  │ Continuous    │ Float64                          │\n",
       "│ class       │ Multiclass{3} │ CategoricalValue{String, UInt32} │\n",
       "└─────────────┴───────────────┴──────────────────────────────────┘\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema do dataframe (metadados)\n",
    "schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalArrays.CategoricalValue{String, UInt32}[\"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-setosa\", \"Iris-virginica\"  …  \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-setosa\"], \u001b[1m150×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────\n",
       "   1 │         6.7         3.3          5.7         2.1\n",
       "   2 │         5.7         2.8          4.1         1.3\n",
       "   3 │         7.2         3.0          5.8         1.6\n",
       "   4 │         4.4         2.9          1.4         0.2\n",
       "   5 │         5.6         2.5          3.9         1.1\n",
       "   6 │         6.5         3.0          5.2         2.0\n",
       "   7 │         4.4         3.0          1.3         0.2\n",
       "   8 │         6.1         2.9          4.7         1.4\n",
       "   9 │         5.4         3.9          1.7         0.4\n",
       "  10 │         4.9         2.5          4.5         1.7\n",
       "  11 │         6.3         2.5          4.9         1.5\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮\n",
       " 141 │         6.4         2.7          5.3         1.9\n",
       " 142 │         6.8         3.2          5.9         2.3\n",
       " 143 │         6.9         3.1          5.4         2.1\n",
       " 144 │         6.1         2.8          4.0         1.3\n",
       " 145 │         6.7         2.5          5.8         1.8\n",
       " 146 │         5.0         3.5          1.3         0.3\n",
       " 147 │         7.6         3.0          6.6         2.1\n",
       " 148 │         6.3         2.5          5.0         1.9\n",
       " 149 │         5.1         3.8          1.6         0.2\n",
       " 150 │         5.0         3.6          1.4         0.2\n",
       "\u001b[36m                                        129 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrai x e y do dataframe\n",
    "# x = variáveis preditoras\n",
    "# y = variavel alvo (class)\n",
    "y, X = unpack(df, ==(:class), rng = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Multiclass{3}} (alias for AbstractArray{Multiclass{3}, 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema da variável y\n",
    "scitype(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "\\end{verbatim}\n",
       "Horizontally split any Tables.jl compatible \\texttt{table} into smaller tables or vectors by making column selections determined by the predicates \\texttt{f1}, \\texttt{f2}, ..., \\texttt{fk}. Selection from the column names is without replacement. A \\emph{predicate} is any object \\texttt{f} such that \\texttt{f(name)} is \\texttt{true} or \\texttt{false} for each column \\texttt{name::Symbol} of \\texttt{table}.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "Z, XY, W = unpack(table, ==(:z), !=(:w))\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "\\end{verbatim}\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless \\texttt{wrap\\_singles=true}.\n",
       "\n",
       "If \\texttt{coerce\\_options} are specified then \\texttt{table} is first replaced with \\texttt{coerce(table, coerce\\_options)}. See \\href{@ref}{\\texttt{ScientificTypes.coerce}} for details.\n",
       "\n",
       "If \\texttt{shuffle=true} then the rows of \\texttt{table} are first shuffled, using the global RNG, unless \\texttt{rng} is specified; if \\texttt{rng} is an integer, it specifies the seed of an automatically generated Mersenne twister. If \\texttt{rng} is specified then \\texttt{shuffle=true} is implicit.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "```\n",
       "\n",
       "Horizontally split any Tables.jl compatible `table` into smaller tables or vectors by making column selections determined by the predicates `f1`, `f2`, ..., `fk`. Selection from the column names is without replacement. A *predicate* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "```\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "Z, XY, W = unpack(table, ==(:z), !=(:w))\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "```\n",
       "\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n",
       "\n",
       "If `coerce_options` are specified then `table` is first replaced with `coerce(table, coerce_options)`. See [`ScientificTypes.coerce`](@ref) for details.\n",
       "\n",
       "If `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n"
      ],
      "text/plain": [
       "\u001b[36m  unpack(table, f1, f2, ... fk;\u001b[39m\n",
       "\u001b[36m         wrap_singles=false,\u001b[39m\n",
       "\u001b[36m         shuffle=false,\u001b[39m\n",
       "\u001b[36m         rng::Union{AbstractRNG,Int,Nothing}=nothing,\u001b[39m\n",
       "\u001b[36m         coerce_options...)\u001b[39m\n",
       "\n",
       "  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables or\n",
       "  vectors by making column selections determined by the predicates \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m,\n",
       "  ..., \u001b[36mfk\u001b[39m. Selection from the column names is without replacement. A \u001b[4mpredicate\u001b[24m\n",
       "  is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column\n",
       "  \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m.\n",
       "\n",
       "  Returns a tuple of tables/vectors with length one greater than the number of\n",
       "  supplied predicates, with the last component including all previously\n",
       "  unselected columns.\n",
       "\n",
       "\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n",
       "\u001b[36m  2×4 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y     z        w\u001b[39m\n",
       "\u001b[36m       │ Int64  Char  Float64  String\u001b[39m\n",
       "\u001b[36m  ─────┼──────────────────────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a        10.0  A\u001b[39m\n",
       "\u001b[36m     2 │     2  b        20.0  B\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Z, XY, W = unpack(table, ==(:z), !=(:w))\u001b[39m\n",
       "\u001b[36m  julia> Z\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   10.0\u001b[39m\n",
       "\u001b[36m   20.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> XY\u001b[39m\n",
       "\u001b[36m  2×2 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y\u001b[39m\n",
       "\u001b[36m       │ Int64  Char\u001b[39m\n",
       "\u001b[36m  ─────┼─────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a\u001b[39m\n",
       "\u001b[36m     2 │     2  b\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> W  # the column(s) left over\u001b[39m\n",
       "\u001b[36m  2-element Vector{String}:\u001b[39m\n",
       "\u001b[36m   \"A\"\u001b[39m\n",
       "\u001b[36m   \"B\"\u001b[39m\n",
       "\n",
       "  Whenever a returned table contains a single column, it is converted to a\n",
       "  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n",
       "\n",
       "  If \u001b[36mcoerce_options\u001b[39m are specified then \u001b[36mtable\u001b[39m is first replaced with\n",
       "  \u001b[36mcoerce(table, coerce_options)\u001b[39m. See \u001b[36mScientificTypes.coerce\u001b[39m for details.\n",
       "\n",
       "  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n",
       "  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n",
       "  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n",
       "  \u001b[36mshuffle=true\u001b[39m is implicit."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documentação da função\n",
    "@doc unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = BM25Transformer, package_name = MLJText, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " ⋮\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pesquisa por todos os modelos de ML disponíveis\n",
    "all_models = models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = LinearRegressor, package_name = GLM, ... )\n",
       " (name = LinearRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = LinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = LinearRegressor, package_name = ScikitLearn, ... )\n",
       " (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = SVMLinearRegressor, package_name = ScikitLearn, ... )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pesquisa por todos os modelos de ML disponíveis, sendo do tipo Regressão Linear\n",
    "some_models = models(\"LinearRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[35mLinear regressor (OLS) with a Normal model.\u001b[39m\n",
       "\u001b[35m→ based on [GLM](https://github.com/JuliaStats/GLM.jl).\u001b[39m\n",
       "\u001b[35m→ do `@load LinearRegressor pkg=\"GLM\"` to use the model.\u001b[39m\n",
       "\u001b[35m→ do `?LinearRegressor` for documentation.\u001b[39m\n",
       "(name = \"LinearRegressor\",\n",
       " package_name = \"GLM\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " deep_properties = (),\n",
       " docstring = \"\"\"\n",
       "             Linear regressor (OLS) with a Normal model.\n",
       "             → based on [GLM](https://github.com/JuliaStats/GLM.jl).\n",
       "             → do `@load LinearRegressor pkg=\"GLM\"` to use the model.\n",
       "             → do `?LinearRegressor` for documentation.\"\"\",\n",
       " fit_data_scitype =\n",
       "     Tuple{Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:Continuous), AbstractVector{Continuous}},\n",
       " hyperparameter_ranges = (nothing, nothing, nothing),\n",
       " hyperparameter_types = (\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\"),\n",
       " hyperparameters = (:fit_intercept, :allowrankdeficient, :offsetcol),\n",
       " implemented_methods = [:fit, :fitted_params, :predict, :predict_mean],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = nothing,\n",
       " load_path = \"MLJGLMInterface.LinearRegressor\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/JuliaStats/GLM.jl\",\n",
       " package_uuid = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\",\n",
       " predict_scitype = AbstractVector{ScientificTypesBase.Density{Continuous}},\n",
       " prediction_type = :probabilistic,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = false,\n",
       " supports_weights = false,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype =\n",
       "     Table{_s28} where _s28<:(AbstractVector{_s29} where _s29<:Continuous),\n",
       " target_scitype = AbstractVector{Continuous},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza os detalhes de um dos modelos\n",
    "meta = some_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Continuous} (alias for AbstractArray{Continuous, 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo de variável alvo do modelo anterior\n",
    "targetscitype = meta.target_scitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica se podemos usar o modelo anterior com a nossa variável alvo\n",
    "# Não podemos, pois o modelo é de regressão e nossa variável é para classificação\n",
    "scitype(y) <: targetscitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_julia_classifiers (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para filtrar modelos de classificação\n",
    "filter_julia_classifiers(meta) = AbstractVector{Finite} <: meta.target_scitype && meta.is_pure_julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n",
       " (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = PegasosClassifier, package_name = BetaML, ... )\n",
       " (name = PerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtra os modelos de classificação\n",
    "models(filter_julia_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quais modelos podem ser usados com nossas variáveis x e y?\n",
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Dropbox/DSA/DSMLJulia/MLJulia/env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Dropbox/DSA/DSMLJulia/MLJulia/env/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote MLJFlux\n",
    "Pkg.add(\"MLJFlux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /Users/dmpm/.julia/packages/MLJModels/kwZnx/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJFlux.NeuralNetworkClassifier"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o algoritmo de rede neural\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "    builder = Short(\n",
       "            n_hidden = 0,\n",
       "            dropout = 0.5,\n",
       "            σ = NNlib.σ),\n",
       "    finaliser = NNlib.softmax,\n",
       "    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()),\n",
       "    loss = Flux.Losses.crossentropy,\n",
       "    epochs = 10,\n",
       "    batch_size = 1,\n",
       "    lambda = 0.0,\n",
       "    alpha = 0.0,\n",
       "    rng = Random._GLOBAL_RNG(),\n",
       "    optimiser_changes_trigger_retraining = false,\n",
       "    acceleration = CPU1{Nothing}(nothing))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o modelo\n",
    "model = NeuralNetworkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n",
       "(name = \"NeuralNetworkClassifier\",\n",
       " package_name = \"MLJFlux\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " deep_properties = (:optimiser, :builder),\n",
       " docstring =\n",
       "     \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \",\n",
       " fit_data_scitype =\n",
       "     Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}},\n",
       " hyperparameter_ranges = (nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing),\n",
       " hyperparameter_types = (\"MLJFlux.Short\",\n",
       "                         \"typeof(NNlib.softmax)\",\n",
       "                         \"Flux.Optimise.ADAM\",\n",
       "                         \"typeof(Flux.Losses.crossentropy)\",\n",
       "                         \"Int64\",\n",
       "                         \"Int64\",\n",
       "                         \"Float64\",\n",
       "                         \"Float64\",\n",
       "                         \"Union{Int64, Random.AbstractRNG}\",\n",
       "                         \"Bool\",\n",
       "                         \"ComputationalResources.AbstractResource\"),\n",
       " hyperparameters = (:builder,\n",
       "                    :finaliser,\n",
       "                    :optimiser,\n",
       "                    :loss,\n",
       "                    :epochs,\n",
       "                    :batch_size,\n",
       "                    :lambda,\n",
       "                    :alpha,\n",
       "                    :rng,\n",
       "                    :optimiser_changes_trigger_retraining,\n",
       "                    :acceleration),\n",
       " implemented_methods = [],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = :epochs,\n",
       " load_path = \"MLJFlux.NeuralNetworkClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n",
       " package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n",
       " predict_scitype =\n",
       "     AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:Finite},\n",
       " prediction_type = :probabilistic,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = true,\n",
       " supports_weights = false,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype = Table{<:AbstractVector{<:Continuous}},\n",
       " target_scitype = AbstractVector{<:Finite},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info do modelo\n",
    "info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em MLJ um *modelo* é apenas uma estrutura contendo hiperparâmetros. Um modelo não armazena parâmetros *aprendidos* e os modelos são mutáveis. Para armazenar os parâmetros aprendidos usamos uma *machine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de epochs para treinar o modelo\n",
    "model.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica se o modelo está pronto para ser treinado em 30 epochs\n",
    "NeuralNetworkClassifier(epochs = 30) == model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n",
       "  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n",
       "  args: \n",
       "    1:\tSource @470 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @591 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o objeto que vai armazenar o modelo treinado (machine)\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma máquina (mach) armazena parâmetros *aprendidos*, entre outras coisas. Treinamos esta máquina em 70% dos dados e avaliamos em 30% de dados de validação. Vamos começar dividindo todos os índices de linha em subconjuntos de `train` e `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide os dados em treino e teste\n",
    "train, test = partition(1:length(y), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /Users/dmpm/.julia/packages/MLJBase/hHa7b/src/machines.jl:464\n",
      "┌ Info: Loss is 1.179\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.167\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.175\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.105\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.11\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.072\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.07\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.052\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.064\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.027\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.049\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.0\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.041\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9824\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.012\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.016\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9726\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9855\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9213\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9583\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9388\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9219\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9196\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9506\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8961\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8921\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8729\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8608\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8859\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8634\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n",
       "  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n",
       "  args: \n",
       "    1:\tSource @470 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @591 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.286, Iris-versicolor=>0.356, Iris-virginica=>0.358)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.252, Iris-versicolor=>0.358, Iris-virginica=>0.39)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.64, Iris-versicolor=>0.24, Iris-virginica=>0.119)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.237, Iris-versicolor=>0.357, Iris-virginica=>0.405)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.234, Iris-versicolor=>0.359, Iris-virginica=>0.407)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.609, Iris-versicolor=>0.256, Iris-virginica=>0.135)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.272, Iris-versicolor=>0.358, Iris-virginica=>0.37)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.276, Iris-versicolor=>0.355, Iris-virginica=>0.368)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.25, Iris-versicolor=>0.356, Iris-virginica=>0.394)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.295, Iris-versicolor=>0.355, Iris-virginica=>0.35)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.247, Iris-versicolor=>0.358, Iris-virginica=>0.395)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.272, Iris-versicolor=>0.357, Iris-virginica=>0.371)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.259, Iris-versicolor=>0.359, Iris-virginica=>0.382)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.254, Iris-versicolor=>0.359, Iris-virginica=>0.387)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.646, Iris-versicolor=>0.237, Iris-virginica=>0.117)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.241, Iris-versicolor=>0.358, Iris-virginica=>0.401)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.236, Iris-versicolor=>0.359, Iris-virginica=>0.406)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.241, Iris-versicolor=>0.359, Iris-virginica=>0.401)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.289, Iris-versicolor=>0.355, Iris-virginica=>0.357)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.238, Iris-versicolor=>0.357, Iris-virginica=>0.405)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.667, Iris-versicolor=>0.228, Iris-virginica=>0.106)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.233, Iris-versicolor=>0.358, Iris-virginica=>0.409)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.242, Iris-versicolor=>0.358, Iris-virginica=>0.4)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.674, Iris-versicolor=>0.224, Iris-virginica=>0.102)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.675, Iris-versicolor=>0.223, Iris-virginica=>0.102)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsões com o modelo treinado usando dados de teste\n",
    "yhat = predict(mach, rows = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.286, Iris-versicolor=>0.356, Iris-virginica=>0.358)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.252, Iris-versicolor=>0.358, Iris-virginica=>0.39)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.64, Iris-versicolor=>0.24, Iris-virginica=>0.119)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.237, Iris-versicolor=>0.357, Iris-virginica=>0.405)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.234, Iris-versicolor=>0.359, Iris-virginica=>0.407)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza algumas previsões\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estrutura do modelo\n",
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(training_losses = [1.1916542474221592, 1.1792929777262444, 1.167383391861352, 1.1751527044691115, 1.1046274830841445, 1.109757749812602, 1.0718096235218189, 1.0703001067017914, 1.052485879483554, 1.0639329325583853  …  0.9388055246013759, 0.9219160180185467, 0.9196188445574429, 0.9506421705431342, 0.8960978700171252, 0.8920626823395512, 0.8728878892380918, 0.8607509396337714, 0.8859277583105882, 0.8633512041400969],)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relatório do erro durante o treinamento\n",
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909580465978443"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula o erro médio\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alteramos um hiperparâmetro do modelo\n",
    "model.optimiser.eta = model.optimiser.eta * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n",
       "  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n",
       "  args: \n",
       "    1:\tSource @032 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @914 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recriamos a machine\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /Users/dmpm/.julia/packages/MLJBase/hHa7b/src/machines.jl:464\n",
      "┌ Info: Loss is 1.268\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.113\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 1.008\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9874\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.9201\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8574\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.876\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8642\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8419\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8354\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.839\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.8136\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7747\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7752\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7991\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7657\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6936\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.795\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7096\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6496\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6617\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6945\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7164\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6646\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.667\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6259\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.7001\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6448\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6329\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n",
      "┌ Info: Loss is 0.6798\n",
      "└ @ MLJFlux /Users/dmpm/.julia/packages/MLJFlux/ex3rh/src/core.jl:127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n",
       "  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n",
       "  args: \n",
       "    1:\tSource @032 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @914 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamos novamente o modelo\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5296786936051138"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erro médio do modelo\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericRange(1 ≤ epochs ≤ 200; origin=100.5, unit=99.5) on log10 scale"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Range de valores para criar a curva de aprendizado\n",
    "r = range(model, :epochs, lower = 1, upper = 200, scale = :log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "└ @ MLJBase /Users/dmpm/.julia/packages/MLJBase/hHa7b/src/machines.jl:464\n",
      "┌ Info: Attempting to evaluate 25 models.\n",
      "└ @ MLJTuning /Users/dmpm/.julia/packages/MLJTuning/Al9yX/src/tuned_models.jl:680\n",
      "\u001b[33mEvaluating over 25 metamodels: 100%[=========================] Time: 0:00:06\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"epochs\",\n",
       " parameter_scale = :log10,\n",
       " parameter_values = [1, 2, 3, 4, 5, 6, 7, 9, 11, 13  …  39, 46, 56, 67, 80, 96, 116, 139, 167, 200],\n",
       " measurements = [1.1036786600899762, 1.0973908999985746, 1.0902729010991856, 1.0642236835422005, 1.0164699782007753, 0.9574910840975024, 0.908876286879632, 0.838850725264529, 0.79979391121836, 0.7473548344140858  …  0.5141964029625843, 0.4962720657651603, 0.4525371324092868, 0.4261048604178746, 0.39685405490873776, 0.36977095483638905, 0.3477984184864085, 0.316135438376356, 0.2909616694811594, 0.27700766971664026],)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Curva de aprendizado durante o treinamento\n",
    "curva_aprendizado = learning_curve(model, \n",
    "                                   X, \n",
    "                                   y,\n",
    "                                   range = r,\n",
    "                                   resampling = Holdout(fraction_train = 0.7),\n",
    "                                   measure = cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip850\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip850)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip851\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip850)\" d=\"\n",
       "M219.866 1410.9 L2352.76 1410.9 L2352.76 47.2441 L219.866 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip852\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  280.231,1410.9 280.231,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1154.69,1410.9 1154.69,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2029.15,1410.9 2029.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1410.9 2352.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,1410.9 280.231,1392 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1154.69,1410.9 1154.69,1392 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2029.15,1410.9 2029.15,1392 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip850)\" d=\"M245.214 1485.02 L252.853 1485.02 L252.853 1458.66 L244.543 1460.32 L244.543 1456.06 L252.806 1454.4 L257.482 1454.4 L257.482 1485.02 L265.121 1485.02 L265.121 1488.96 L245.214 1488.96 L245.214 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M284.566 1457.48 Q280.954 1457.48 279.126 1461.04 Q277.32 1464.58 277.32 1471.71 Q277.32 1478.82 279.126 1482.38 Q280.954 1485.92 284.566 1485.92 Q288.2 1485.92 290.005 1482.38 Q291.834 1478.82 291.834 1471.71 Q291.834 1464.58 290.005 1461.04 Q288.2 1457.48 284.566 1457.48 M284.566 1453.77 Q290.376 1453.77 293.431 1458.38 Q296.51 1462.96 296.51 1471.71 Q296.51 1480.44 293.431 1485.04 Q290.376 1489.63 284.566 1489.63 Q278.755 1489.63 275.677 1485.04 Q272.621 1480.44 272.621 1471.71 Q272.621 1462.96 275.677 1458.38 Q278.755 1453.77 284.566 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M306.215 1435.97 Q303.281 1435.97 301.795 1438.86 Q300.328 1441.74 300.328 1447.53 Q300.328 1453.31 301.795 1456.2 Q303.281 1459.08 306.215 1459.08 Q309.168 1459.08 310.635 1456.2 Q312.12 1453.31 312.12 1447.53 Q312.12 1441.74 310.635 1438.86 Q309.168 1435.97 306.215 1435.97 M306.215 1432.96 Q310.935 1432.96 313.418 1436.7 Q315.92 1440.43 315.92 1447.53 Q315.92 1454.62 313.418 1458.37 Q310.935 1462.09 306.215 1462.09 Q301.494 1462.09 298.993 1458.37 Q296.51 1454.62 296.51 1447.53 Q296.51 1440.43 298.993 1436.7 Q301.494 1432.96 306.215 1432.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1121.02 1485.02 L1128.66 1485.02 L1128.66 1458.66 L1120.35 1460.32 L1120.35 1456.06 L1128.61 1454.4 L1133.29 1454.4 L1133.29 1485.02 L1140.93 1485.02 L1140.93 1488.96 L1121.02 1488.96 L1121.02 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1160.37 1457.48 Q1156.76 1457.48 1154.93 1461.04 Q1153.13 1464.58 1153.13 1471.71 Q1153.13 1478.82 1154.93 1482.38 Q1156.76 1485.92 1160.37 1485.92 Q1164.01 1485.92 1165.81 1482.38 Q1167.64 1478.82 1167.64 1471.71 Q1167.64 1464.58 1165.81 1461.04 Q1164.01 1457.48 1160.37 1457.48 M1160.37 1453.77 Q1166.18 1453.77 1169.24 1458.38 Q1172.32 1462.96 1172.32 1471.71 Q1172.32 1480.44 1169.24 1485.04 Q1166.18 1489.63 1160.37 1489.63 Q1154.56 1489.63 1151.48 1485.04 Q1148.43 1480.44 1148.43 1471.71 Q1148.43 1462.96 1151.48 1458.38 Q1154.56 1453.77 1160.37 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1172.86 1458.35 L1179.07 1458.35 L1179.07 1436.93 L1172.32 1438.28 L1172.32 1434.82 L1179.03 1433.47 L1182.83 1433.47 L1182.83 1458.35 L1189.04 1458.35 L1189.04 1461.55 L1172.86 1461.55 L1172.86 1458.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1994.92 1485.02 L2002.56 1485.02 L2002.56 1458.66 L1994.25 1460.32 L1994.25 1456.06 L2002.52 1454.4 L2007.19 1454.4 L2007.19 1485.02 L2014.83 1485.02 L2014.83 1488.96 L1994.92 1488.96 L1994.92 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M2034.28 1457.48 Q2030.67 1457.48 2028.84 1461.04 Q2027.03 1464.58 2027.03 1471.71 Q2027.03 1478.82 2028.84 1482.38 Q2030.67 1485.92 2034.28 1485.92 Q2037.91 1485.92 2039.72 1482.38 Q2041.55 1478.82 2041.55 1471.71 Q2041.55 1464.58 2039.72 1461.04 Q2037.91 1457.48 2034.28 1457.48 M2034.28 1453.77 Q2040.09 1453.77 2043.14 1458.38 Q2046.22 1462.96 2046.22 1471.71 Q2046.22 1480.44 2043.14 1485.04 Q2040.09 1489.63 2034.28 1489.63 Q2028.47 1489.63 2025.39 1485.04 Q2022.33 1480.44 2022.33 1471.71 Q2022.33 1462.96 2025.39 1458.38 Q2028.47 1453.77 2034.28 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M2050.79 1458.35 L2064.05 1458.35 L2064.05 1461.55 L2046.22 1461.55 L2046.22 1458.35 Q2048.38 1456.11 2052.11 1452.35 Q2055.85 1448.57 2056.81 1447.48 Q2058.63 1445.43 2059.35 1444.02 Q2060.08 1442.59 2060.08 1441.22 Q2060.08 1438.98 2058.5 1437.57 Q2056.94 1436.16 2054.42 1436.16 Q2052.63 1436.16 2050.64 1436.78 Q2048.67 1437.4 2046.41 1438.66 L2046.41 1434.82 Q2048.7 1433.9 2050.7 1433.43 Q2052.69 1432.96 2054.35 1432.96 Q2058.71 1432.96 2061.3 1435.14 Q2063.9 1437.32 2063.9 1440.97 Q2063.9 1442.7 2063.24 1444.26 Q2062.6 1445.8 2060.89 1447.91 Q2060.42 1448.46 2057.9 1451.07 Q2055.38 1453.67 2050.79 1458.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1206.5 1554.9 L1206.5 1557.76 L1179.57 1557.76 Q1179.96 1563.81 1183.2 1566.99 Q1186.48 1570.14 1192.31 1570.14 Q1195.68 1570.14 1198.83 1569.32 Q1202.01 1568.49 1205.13 1566.83 L1205.13 1572.37 Q1201.98 1573.71 1198.67 1574.41 Q1195.36 1575.11 1191.96 1575.11 Q1183.43 1575.11 1178.43 1570.14 Q1173.46 1565.18 1173.46 1556.71 Q1173.46 1547.96 1178.17 1542.83 Q1182.92 1537.68 1190.94 1537.68 Q1198.13 1537.68 1202.3 1542.33 Q1206.5 1546.94 1206.5 1554.9 M1200.64 1553.18 Q1200.58 1548.37 1197.94 1545.51 Q1195.33 1542.64 1191 1542.64 Q1186.1 1542.64 1183.14 1545.41 Q1180.21 1548.18 1179.77 1553.21 L1200.64 1553.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1221.78 1568.84 L1221.78 1587.74 L1215.89 1587.74 L1215.89 1538.54 L1221.78 1538.54 L1221.78 1543.95 Q1223.62 1540.77 1226.43 1539.24 Q1229.26 1537.68 1233.17 1537.68 Q1239.67 1537.68 1243.71 1542.83 Q1247.78 1547.99 1247.78 1556.39 Q1247.78 1564.8 1243.71 1569.95 Q1239.67 1575.11 1233.17 1575.11 Q1229.26 1575.11 1226.43 1573.58 Q1223.62 1572.02 1221.78 1568.84 M1241.7 1556.39 Q1241.7 1549.93 1239.03 1546.27 Q1236.39 1542.58 1231.74 1542.58 Q1227.09 1542.58 1224.42 1546.27 Q1221.78 1549.93 1221.78 1556.39 Q1221.78 1562.85 1224.42 1566.55 Q1227.09 1570.21 1231.74 1570.21 Q1236.39 1570.21 1239.03 1566.55 Q1241.7 1562.85 1241.7 1556.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1271.3 1542.64 Q1266.59 1542.64 1263.86 1546.34 Q1261.12 1550 1261.12 1556.39 Q1261.12 1562.79 1263.82 1566.48 Q1266.56 1570.14 1271.3 1570.14 Q1275.98 1570.14 1278.72 1566.45 Q1281.46 1562.76 1281.46 1556.39 Q1281.46 1550.06 1278.72 1546.37 Q1275.98 1542.64 1271.3 1542.64 M1271.3 1537.68 Q1278.94 1537.68 1283.3 1542.64 Q1287.66 1547.61 1287.66 1556.39 Q1287.66 1565.15 1283.3 1570.14 Q1278.94 1575.11 1271.3 1575.11 Q1263.63 1575.11 1259.27 1570.14 Q1254.94 1565.15 1254.94 1556.39 Q1254.94 1547.61 1259.27 1542.64 Q1263.63 1537.68 1271.3 1537.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1323.03 1539.91 L1323.03 1545.38 Q1320.54 1544.01 1318.03 1543.34 Q1315.55 1542.64 1313 1542.64 Q1307.3 1542.64 1304.15 1546.27 Q1301 1549.87 1301 1556.39 Q1301 1562.92 1304.15 1566.55 Q1307.3 1570.14 1313 1570.14 Q1315.55 1570.14 1318.03 1569.47 Q1320.54 1568.77 1323.03 1567.41 L1323.03 1572.82 Q1320.57 1573.96 1317.93 1574.54 Q1315.32 1575.11 1312.36 1575.11 Q1304.31 1575.11 1299.57 1570.05 Q1294.83 1564.99 1294.83 1556.39 Q1294.83 1547.67 1299.6 1542.68 Q1304.41 1537.68 1312.74 1537.68 Q1315.45 1537.68 1318.03 1538.25 Q1320.61 1538.79 1323.03 1539.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1362.84 1552.67 L1362.84 1574.19 L1356.99 1574.19 L1356.99 1552.86 Q1356.99 1547.8 1355.01 1545.29 Q1353.04 1542.77 1349.09 1542.77 Q1344.35 1542.77 1341.61 1545.79 Q1338.88 1548.82 1338.88 1554.04 L1338.88 1574.19 L1332.99 1574.19 L1332.99 1524.66 L1338.88 1524.66 L1338.88 1544.08 Q1340.98 1540.86 1343.81 1539.27 Q1346.67 1537.68 1350.4 1537.68 Q1356.54 1537.68 1359.69 1541.5 Q1362.84 1545.29 1362.84 1552.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M1397.25 1539.59 L1397.25 1545.13 Q1394.77 1543.85 1392.09 1543.22 Q1389.42 1542.58 1386.56 1542.58 Q1382.19 1542.58 1380 1543.92 Q1377.83 1545.25 1377.83 1547.93 Q1377.83 1549.96 1379.39 1551.14 Q1380.95 1552.29 1385.66 1553.34 L1387.67 1553.78 Q1393.91 1555.12 1396.52 1557.57 Q1399.16 1559.99 1399.16 1564.35 Q1399.16 1569.32 1395.21 1572.21 Q1391.3 1575.11 1384.42 1575.11 Q1381.56 1575.11 1378.44 1574.54 Q1375.35 1573.99 1371.91 1572.88 L1371.91 1566.83 Q1375.16 1568.52 1378.31 1569.38 Q1381.46 1570.21 1384.55 1570.21 Q1388.69 1570.21 1390.92 1568.81 Q1393.14 1567.37 1393.14 1564.8 Q1393.14 1562.41 1391.52 1561.14 Q1389.93 1559.86 1384.49 1558.68 L1382.45 1558.21 Q1377.01 1557.06 1374.59 1554.71 Q1372.17 1552.32 1372.17 1548.18 Q1372.17 1543.15 1375.73 1540.42 Q1379.3 1537.68 1385.85 1537.68 Q1389.1 1537.68 1391.97 1538.16 Q1394.83 1538.63 1397.25 1539.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1180.9 2352.76,1180.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,869.662 2352.76,869.662 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,558.422 2352.76,558.422 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip852)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,247.183 2352.76,247.183 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1410.9 219.866,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1180.9 238.764,1180.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,869.662 238.764,869.662 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,558.422 238.764,558.422 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,247.183 238.764,247.183 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip850)\" d=\"M126.205 1166.7 Q122.593 1166.7 120.765 1170.27 Q118.959 1173.81 118.959 1180.94 Q118.959 1188.04 120.765 1191.61 Q122.593 1195.15 126.205 1195.15 Q129.839 1195.15 131.644 1191.61 Q133.473 1188.04 133.473 1180.94 Q133.473 1173.81 131.644 1170.27 Q129.839 1166.7 126.205 1166.7 M126.205 1163 Q132.015 1163 135.07 1167.6 Q138.149 1172.19 138.149 1180.94 Q138.149 1189.66 135.07 1194.27 Q132.015 1198.85 126.205 1198.85 Q120.394 1198.85 117.316 1194.27 Q114.26 1189.66 114.26 1180.94 Q114.26 1172.19 117.316 1167.6 Q120.394 1163 126.205 1163 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M146.366 1192.3 L151.251 1192.3 L151.251 1198.18 L146.366 1198.18 L146.366 1192.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M174.283 1167.7 L162.477 1186.15 L174.283 1186.15 L174.283 1167.7 M173.056 1163.62 L178.936 1163.62 L178.936 1186.15 L183.866 1186.15 L183.866 1190.03 L178.936 1190.03 L178.936 1198.18 L174.283 1198.18 L174.283 1190.03 L158.681 1190.03 L158.681 1185.52 L173.056 1163.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M126.529 855.461 Q122.918 855.461 121.089 859.026 Q119.283 862.567 119.283 869.697 Q119.283 876.803 121.089 880.368 Q122.918 883.91 126.529 883.91 Q130.163 883.91 131.968 880.368 Q133.797 876.803 133.797 869.697 Q133.797 862.567 131.968 859.026 Q130.163 855.461 126.529 855.461 M126.529 851.757 Q132.339 851.757 135.394 856.364 Q138.473 860.947 138.473 869.697 Q138.473 878.424 135.394 883.03 Q132.339 887.614 126.529 887.614 Q120.718 887.614 117.64 883.03 Q114.584 878.424 114.584 869.697 Q114.584 860.947 117.64 856.364 Q120.718 851.757 126.529 851.757 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M146.691 881.063 L151.575 881.063 L151.575 886.942 L146.691 886.942 L146.691 881.063 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M172.339 867.799 Q169.19 867.799 167.339 869.952 Q165.51 872.104 165.51 875.854 Q165.51 879.581 167.339 881.757 Q169.19 883.91 172.339 883.91 Q175.487 883.91 177.315 881.757 Q179.167 879.581 179.167 875.854 Q179.167 872.104 177.315 869.952 Q175.487 867.799 172.339 867.799 M181.621 853.146 L181.621 857.405 Q179.862 856.572 178.056 856.132 Q176.274 855.692 174.514 855.692 Q169.885 855.692 167.431 858.817 Q165.001 861.942 164.653 868.262 Q166.019 866.248 168.079 865.183 Q170.139 864.095 172.616 864.095 Q177.825 864.095 180.834 867.267 Q183.866 870.415 183.866 875.854 Q183.866 881.178 180.718 884.396 Q177.57 887.614 172.339 887.614 Q166.343 887.614 163.172 883.03 Q160.001 878.424 160.001 869.697 Q160.001 861.503 163.89 856.642 Q167.778 851.757 174.329 851.757 Q176.089 851.757 177.871 852.105 Q179.676 852.452 181.621 853.146 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M126.783 544.221 Q123.172 544.221 121.343 547.786 Q119.538 551.328 119.538 558.457 Q119.538 565.564 121.343 569.128 Q123.172 572.67 126.783 572.67 Q130.417 572.67 132.223 569.128 Q134.052 565.564 134.052 558.457 Q134.052 551.328 132.223 547.786 Q130.417 544.221 126.783 544.221 M126.783 540.518 Q132.593 540.518 135.649 545.124 Q138.728 549.707 138.728 558.457 Q138.728 567.184 135.649 571.79 Q132.593 576.374 126.783 576.374 Q120.973 576.374 117.894 571.79 Q114.839 567.184 114.839 558.457 Q114.839 549.707 117.894 545.124 Q120.973 540.518 126.783 540.518 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M146.945 569.823 L151.829 569.823 L151.829 575.702 L146.945 575.702 L146.945 569.823 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M172.014 559.291 Q168.681 559.291 166.76 561.073 Q164.862 562.855 164.862 565.98 Q164.862 569.105 166.76 570.888 Q168.681 572.67 172.014 572.67 Q175.348 572.67 177.269 570.888 Q179.19 569.082 179.19 565.98 Q179.19 562.855 177.269 561.073 Q175.371 559.291 172.014 559.291 M167.339 557.3 Q164.329 556.559 162.64 554.499 Q160.973 552.439 160.973 549.476 Q160.973 545.332 163.913 542.925 Q166.876 540.518 172.014 540.518 Q177.176 540.518 180.116 542.925 Q183.056 545.332 183.056 549.476 Q183.056 552.439 181.366 554.499 Q179.7 556.559 176.714 557.3 Q180.093 558.087 181.968 560.379 Q183.866 562.67 183.866 565.98 Q183.866 571.003 180.788 573.689 Q177.732 576.374 172.014 576.374 Q166.297 576.374 163.218 573.689 Q160.163 571.003 160.163 565.98 Q160.163 562.67 162.061 560.379 Q163.959 558.087 167.339 557.3 M165.626 549.916 Q165.626 552.601 167.292 554.105 Q168.982 555.61 172.014 555.61 Q175.024 555.61 176.714 554.105 Q178.426 552.601 178.426 549.916 Q178.426 547.23 176.714 545.726 Q175.024 544.221 172.014 544.221 Q168.982 544.221 167.292 545.726 Q165.626 547.23 165.626 549.916 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M117.501 260.528 L125.14 260.528 L125.14 234.162 L116.83 235.829 L116.83 231.569 L125.093 229.903 L129.769 229.903 L129.769 260.528 L137.408 260.528 L137.408 264.463 L117.501 264.463 L117.501 260.528 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M146.853 258.583 L151.737 258.583 L151.737 264.463 L146.853 264.463 L146.853 258.583 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M171.922 232.981 Q168.311 232.981 166.482 236.546 Q164.677 240.088 164.677 247.217 Q164.677 254.324 166.482 257.889 Q168.311 261.43 171.922 261.43 Q175.556 261.43 177.362 257.889 Q179.19 254.324 179.19 247.217 Q179.19 240.088 177.362 236.546 Q175.556 232.981 171.922 232.981 M171.922 229.278 Q177.732 229.278 180.788 233.884 Q183.866 238.467 183.866 247.217 Q183.866 255.944 180.788 260.551 Q177.732 265.134 171.922 265.134 Q166.112 265.134 163.033 260.551 Q159.978 255.944 159.978 247.217 Q159.978 238.467 163.033 233.884 Q166.112 229.278 171.922 229.278 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M16.4842 1228.95 L16.4842 1198.91 L21.895 1198.91 L21.895 1222.52 L35.9632 1222.52 L35.9632 1199.89 L41.3741 1199.89 L41.3741 1222.52 L58.5933 1222.52 L58.5933 1198.33 L64.0042 1198.33 L64.0042 1228.95 L16.4842 1228.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M33.8307 1167.37 Q33.2578 1168.35 33.0032 1169.53 Q32.7167 1170.68 32.7167 1172.08 Q32.7167 1177.04 35.9632 1179.71 Q39.1779 1182.36 45.2253 1182.36 L64.0042 1182.36 L64.0042 1188.24 L28.3562 1188.24 L28.3562 1182.36 L33.8944 1182.36 Q30.6479 1180.51 29.0883 1177.55 Q27.4968 1174.59 27.4968 1170.36 Q27.4968 1169.75 27.5923 1169.02 Q27.656 1168.29 27.8151 1167.4 L33.8307 1167.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M33.8307 1141.71 Q33.2578 1142.7 33.0032 1143.88 Q32.7167 1145.02 32.7167 1146.42 Q32.7167 1151.39 35.9632 1154.06 Q39.1779 1156.7 45.2253 1156.7 L64.0042 1156.7 L64.0042 1162.59 L28.3562 1162.59 L28.3562 1156.7 L33.8944 1156.7 Q30.6479 1154.86 29.0883 1151.9 Q27.4968 1148.94 27.4968 1144.7 Q27.4968 1144.1 27.5923 1143.37 Q27.656 1142.63 27.8151 1141.74 L33.8307 1141.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M32.4621 1123.19 Q32.4621 1127.9 36.1542 1130.64 Q39.8145 1133.37 46.212 1133.37 Q52.6095 1133.37 56.3017 1130.67 Q59.9619 1127.93 59.9619 1123.19 Q59.9619 1118.51 56.2698 1115.77 Q52.5777 1113.03 46.212 1113.03 Q39.8781 1113.03 36.186 1115.77 Q32.4621 1118.51 32.4621 1123.19 M27.4968 1123.19 Q27.4968 1115.55 32.4621 1111.19 Q37.4273 1106.83 46.212 1106.83 Q54.9649 1106.83 59.9619 1111.19 Q64.9272 1115.55 64.9272 1123.19 Q64.9272 1130.86 59.9619 1135.22 Q54.9649 1139.55 46.212 1139.55 Q37.4273 1139.55 32.4621 1135.22 Q27.4968 1130.86 27.4968 1123.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M41.3104 1069.72 L58.7206 1069.72 L58.7206 1059.4 Q58.7206 1054.21 56.5881 1051.73 Q54.4238 1049.22 49.9996 1049.22 Q45.5436 1049.22 43.4429 1051.73 Q41.3104 1054.21 41.3104 1059.4 L41.3104 1069.72 M21.7677 1069.72 L36.0905 1069.72 L36.0905 1060.2 Q36.0905 1055.49 34.34 1053.2 Q32.5576 1050.87 28.9291 1050.87 Q25.3325 1050.87 23.5501 1053.2 Q21.7677 1055.49 21.7677 1060.2 L21.7677 1069.72 M16.4842 1076.14 L16.4842 1059.72 Q16.4842 1052.37 19.5397 1048.39 Q22.5952 1044.41 28.2289 1044.41 Q32.5894 1044.41 35.1675 1046.45 Q37.7456 1048.49 38.3822 1052.43 Q39.4007 1047.69 42.6472 1045.08 Q45.8619 1042.44 50.6998 1042.44 Q57.0655 1042.44 60.5348 1046.77 Q64.0042 1051.1 64.0042 1059.08 L64.0042 1076.14 L16.4842 1076.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M46.0847 1015.48 Q46.0847 1022.58 47.7079 1025.31 Q49.3312 1028.05 53.2461 1028.05 Q56.3653 1028.05 58.2114 1026.01 Q60.0256 1023.95 60.0256 1020.41 Q60.0256 1015.54 56.5881 1012.61 Q53.1188 1009.65 47.3897 1009.65 L46.0847 1009.65 L46.0847 1015.48 M43.6657 1003.8 L64.0042 1003.8 L64.0042 1009.65 L58.5933 1009.65 Q61.8398 1011.66 63.3994 1014.65 Q64.9272 1017.64 64.9272 1021.97 Q64.9272 1027.45 61.8716 1030.69 Q58.7843 1033.91 53.6281 1033.91 Q47.6125 1033.91 44.5569 1029.9 Q41.5014 1025.86 41.5014 1017.87 L41.5014 1009.65 L40.9285 1009.65 Q36.8862 1009.65 34.6901 1012.33 Q32.4621 1014.97 32.4621 1019.78 Q32.4621 1022.83 33.1941 1025.73 Q33.9262 1028.62 35.3903 1031.3 L29.9795 1031.3 Q28.7381 1028.08 28.1334 1025.06 Q27.4968 1022.04 27.4968 1019.17 Q27.4968 1011.44 31.5072 1007.62 Q35.5176 1003.8 43.6657 1003.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M29.4065 969.01 L34.9447 969.01 Q33.6716 971.492 33.035 974.166 Q32.3984 976.84 32.3984 979.704 Q32.3984 984.065 33.7352 986.261 Q35.072 988.425 37.7456 988.425 Q39.7826 988.425 40.9603 986.866 Q42.1061 985.306 43.1565 980.595 L43.6021 978.59 Q44.9389 972.352 47.3897 969.742 Q49.8086 967.1 54.1691 967.1 Q59.1344 967.1 62.0308 971.047 Q64.9272 974.962 64.9272 981.837 Q64.9272 984.701 64.3543 987.82 Q63.8132 990.908 62.6992 994.345 L56.6518 994.345 Q58.3387 991.099 59.198 987.948 Q60.0256 984.797 60.0256 981.709 Q60.0256 977.572 58.6251 975.344 Q57.1929 973.116 54.6147 973.116 Q52.2276 973.116 50.9545 974.739 Q49.6813 976.33 48.5037 981.773 L48.0262 983.81 Q46.8804 989.253 44.5251 991.672 Q42.138 994.091 38.0002 994.091 Q32.9713 994.091 30.2341 990.526 Q27.4968 986.961 27.4968 980.404 Q27.4968 977.158 27.9743 974.293 Q28.4517 971.429 29.4065 969.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M44.7161 927.283 L47.5806 927.283 L47.5806 954.209 Q53.6281 953.828 56.8109 950.581 Q59.9619 947.303 59.9619 941.478 Q59.9619 938.104 59.1344 934.953 Q58.3069 931.77 56.6518 928.651 L62.1899 928.651 Q63.5267 931.802 64.227 935.112 Q64.9272 938.423 64.9272 941.828 Q64.9272 950.358 59.9619 955.355 Q54.9967 960.321 46.5303 960.321 Q37.7774 960.321 32.6531 955.61 Q27.4968 950.868 27.4968 942.847 Q27.4968 935.653 32.1438 931.484 Q36.7589 927.283 44.7161 927.283 M42.9973 933.139 Q38.1912 933.203 35.3266 935.844 Q32.4621 938.454 32.4621 942.783 Q32.4621 947.685 35.2312 950.645 Q38.0002 953.573 43.0292 954.019 L42.9973 933.139 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M46.0847 901.47 Q46.0847 908.567 47.7079 911.305 Q49.3312 914.042 53.2461 914.042 Q56.3653 914.042 58.2114 912.005 Q60.0256 909.936 60.0256 906.403 Q60.0256 901.533 56.5881 898.605 Q53.1188 895.645 47.3897 895.645 L46.0847 895.645 L46.0847 901.47 M43.6657 889.789 L64.0042 889.789 L64.0042 895.645 L58.5933 895.645 Q61.8398 897.65 63.3994 900.642 Q64.9272 903.634 64.9272 907.963 Q64.9272 913.437 61.8716 916.684 Q58.7843 919.898 53.6281 919.898 Q47.6125 919.898 44.5569 915.888 Q41.5014 911.846 41.5014 903.857 L41.5014 895.645 L40.9285 895.645 Q36.8862 895.645 34.6901 898.319 Q32.4621 900.96 32.4621 905.766 Q32.4621 908.822 33.1941 911.718 Q33.9262 914.615 35.3903 917.288 L29.9795 917.288 Q28.7381 914.074 28.1334 911.05 Q27.4968 908.026 27.4968 905.162 Q27.4968 897.427 31.5072 893.608 Q35.5176 889.789 43.6657 889.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M33.7671 854.268 L14.479 854.268 L14.479 848.411 L64.0042 848.411 L64.0042 854.268 L58.657 854.268 Q61.8398 856.114 63.3994 858.947 Q64.9272 861.748 64.9272 865.694 Q64.9272 872.156 59.771 876.23 Q54.6147 880.272 46.212 880.272 Q37.8093 880.272 32.6531 876.23 Q27.4968 872.156 27.4968 865.694 Q27.4968 861.748 29.0564 858.947 Q30.5842 856.114 33.7671 854.268 M46.212 874.224 Q52.6732 874.224 56.3653 871.583 Q60.0256 868.909 60.0256 864.262 Q60.0256 859.615 56.3653 856.942 Q52.6732 854.268 46.212 854.268 Q39.7508 854.268 36.0905 856.942 Q32.3984 859.615 32.3984 864.262 Q32.3984 868.909 36.0905 871.583 Q39.7508 874.224 46.212 874.224 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M32.4621 822.535 Q32.4621 827.245 36.1542 829.983 Q39.8145 832.72 46.212 832.72 Q52.6095 832.72 56.3017 830.015 Q59.9619 827.277 59.9619 822.535 Q59.9619 817.856 56.2698 815.119 Q52.5777 812.382 46.212 812.382 Q39.8781 812.382 36.186 815.119 Q32.4621 817.856 32.4621 822.535 M27.4968 822.535 Q27.4968 814.896 32.4621 810.535 Q37.4273 806.175 46.212 806.175 Q54.9649 806.175 59.9619 810.535 Q64.9272 814.896 64.9272 822.535 Q64.9272 830.206 59.9619 834.566 Q54.9649 838.895 46.212 838.895 Q37.4273 838.895 32.4621 834.566 Q27.4968 830.206 27.4968 822.535 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M42.4881 746.115 L64.0042 746.115 L64.0042 751.971 L42.679 751.971 Q37.6183 751.971 35.1038 753.944 Q32.5894 755.918 32.5894 759.864 Q32.5894 764.607 35.6131 767.344 Q38.6368 770.081 43.8567 770.081 L64.0042 770.081 L64.0042 775.97 L28.3562 775.97 L28.3562 770.081 L33.8944 770.081 Q30.6797 767.981 29.0883 765.148 Q27.4968 762.283 27.4968 758.56 Q27.4968 752.417 31.3163 749.266 Q35.1038 746.115 42.4881 746.115 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M46.0847 718.233 Q46.0847 725.331 47.7079 728.068 Q49.3312 730.805 53.2461 730.805 Q56.3653 730.805 58.2114 728.768 Q60.0256 726.699 60.0256 723.166 Q60.0256 718.296 56.5881 715.368 Q53.1188 712.408 47.3897 712.408 L46.0847 712.408 L46.0847 718.233 M43.6657 706.552 L64.0042 706.552 L64.0042 712.408 L58.5933 712.408 Q61.8398 714.413 63.3994 717.405 Q64.9272 720.397 64.9272 724.726 Q64.9272 730.2 61.8716 733.447 Q58.7843 736.661 53.6281 736.661 Q47.6125 736.661 44.5569 732.651 Q41.5014 728.609 41.5014 720.62 L41.5014 712.408 L40.9285 712.408 Q36.8862 712.408 34.6901 715.082 Q32.4621 717.723 32.4621 722.53 Q32.4621 725.585 33.1941 728.482 Q33.9262 731.378 35.3903 734.052 L29.9795 734.052 Q28.7381 730.837 28.1334 727.813 Q27.4968 724.789 27.4968 721.925 Q27.4968 714.191 31.5072 710.371 Q35.5176 706.552 43.6657 706.552 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M20.1444 637.929 L26.9239 637.929 Q23.9002 641.176 22.4043 644.868 Q20.9083 648.528 20.9083 652.666 Q20.9083 660.814 25.9054 665.143 Q30.8707 669.471 40.2919 669.471 Q49.6813 669.471 54.6784 665.143 Q59.6436 660.814 59.6436 652.666 Q59.6436 648.528 58.1477 644.868 Q56.6518 641.176 53.6281 637.929 L60.3439 637.929 Q62.6355 641.303 63.7814 645.091 Q64.9272 648.847 64.9272 653.048 Q64.9272 663.838 58.3387 670.044 Q51.7183 676.251 40.2919 676.251 Q28.8336 676.251 22.2451 670.044 Q15.6248 663.838 15.6248 653.048 Q15.6248 648.783 16.7706 645.027 Q17.8846 641.24 20.1444 637.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M33.8307 607.597 Q33.2578 608.583 33.0032 609.761 Q32.7167 610.907 32.7167 612.307 Q32.7167 617.273 35.9632 619.946 Q39.1779 622.588 45.2253 622.588 L64.0042 622.588 L64.0042 628.476 L28.3562 628.476 L28.3562 622.588 L33.8944 622.588 Q30.6479 620.742 29.0883 617.782 Q27.4968 614.822 27.4968 610.589 Q27.4968 609.984 27.5923 609.252 Q27.656 608.52 27.8151 607.629 L33.8307 607.597 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M32.4621 589.073 Q32.4621 593.783 36.1542 596.52 Q39.8145 599.258 46.212 599.258 Q52.6095 599.258 56.3017 596.552 Q59.9619 593.815 59.9619 589.073 Q59.9619 584.394 56.2698 581.657 Q52.5777 578.919 46.212 578.919 Q39.8781 578.919 36.186 581.657 Q32.4621 584.394 32.4621 589.073 M27.4968 589.073 Q27.4968 581.434 32.4621 577.073 Q37.4273 572.713 46.212 572.713 Q54.9649 572.713 59.9619 577.073 Q64.9272 581.434 64.9272 589.073 Q64.9272 596.743 59.9619 601.104 Q54.9649 605.432 46.212 605.432 Q37.4273 605.432 32.4621 601.104 Q27.4968 596.743 27.4968 589.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M29.4065 540.279 L34.9447 540.279 Q33.6716 542.762 33.035 545.436 Q32.3984 548.109 32.3984 550.974 Q32.3984 555.334 33.7352 557.53 Q35.072 559.695 37.7456 559.695 Q39.7826 559.695 40.9603 558.135 Q42.1061 556.576 43.1565 551.865 L43.6021 549.86 Q44.9389 543.621 47.3897 541.011 Q49.8086 538.37 54.1691 538.37 Q59.1344 538.37 62.0308 542.316 Q64.9272 546.231 64.9272 553.106 Q64.9272 555.971 64.3543 559.09 Q63.8132 562.177 62.6992 565.615 L56.6518 565.615 Q58.3387 562.368 59.198 559.217 Q60.0256 556.066 60.0256 552.979 Q60.0256 548.841 58.6251 546.613 Q57.1929 544.385 54.6147 544.385 Q52.2276 544.385 50.9545 546.009 Q49.6813 547.6 48.5037 553.043 L48.0262 555.08 Q46.8804 560.522 44.5251 562.941 Q42.138 565.36 38.0002 565.36 Q32.9713 565.36 30.2341 561.796 Q27.4968 558.231 27.4968 551.674 Q27.4968 548.428 27.9743 545.563 Q28.4517 542.698 29.4065 540.279 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M29.4065 506.318 L34.9447 506.318 Q33.6716 508.801 33.035 511.475 Q32.3984 514.148 32.3984 517.013 Q32.3984 521.373 33.7352 523.569 Q35.072 525.734 37.7456 525.734 Q39.7826 525.734 40.9603 524.174 Q42.1061 522.615 43.1565 517.904 L43.6021 515.899 Q44.9389 509.66 47.3897 507.05 Q49.8086 504.409 54.1691 504.409 Q59.1344 504.409 62.0308 508.355 Q64.9272 512.27 64.9272 519.145 Q64.9272 522.01 64.3543 525.129 Q63.8132 528.216 62.6992 531.654 L56.6518 531.654 Q58.3387 528.407 59.198 525.256 Q60.0256 522.105 60.0256 519.018 Q60.0256 514.88 58.6251 512.652 Q57.1929 510.424 54.6147 510.424 Q52.2276 510.424 50.9545 512.048 Q49.6813 513.639 48.5037 519.082 L48.0262 521.119 Q46.8804 526.561 44.5251 528.98 Q42.138 531.399 38.0002 531.399 Q32.9713 531.399 30.2341 527.834 Q27.4968 524.27 27.4968 517.713 Q27.4968 514.466 27.9743 511.602 Q28.4517 508.737 29.4065 506.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M16.4842 474.108 L16.4842 444.062 L21.895 444.062 L21.895 467.679 L35.9632 467.679 L35.9632 445.048 L41.3741 445.048 L41.3741 467.679 L58.5933 467.679 L58.5933 443.489 L64.0042 443.489 L64.0042 474.108 L16.4842 474.108 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M42.4881 403.544 L64.0042 403.544 L64.0042 409.4 L42.679 409.4 Q37.6183 409.4 35.1038 411.374 Q32.5894 413.347 32.5894 417.294 Q32.5894 422.036 35.6131 424.774 Q38.6368 427.511 43.8567 427.511 L64.0042 427.511 L64.0042 433.399 L28.3562 433.399 L28.3562 427.511 L33.8944 427.511 Q30.6797 425.41 29.0883 422.578 Q27.4968 419.713 27.4968 415.989 Q27.4968 409.846 31.3163 406.695 Q35.1038 403.544 42.4881 403.544 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M18.2347 386.07 L28.3562 386.07 L28.3562 374.007 L32.9077 374.007 L32.9077 386.07 L52.2594 386.07 Q56.6199 386.07 57.8613 384.893 Q59.1026 383.683 59.1026 380.023 L59.1026 374.007 L64.0042 374.007 L64.0042 380.023 Q64.0042 386.802 61.4897 389.38 Q58.9434 391.958 52.2594 391.958 L32.9077 391.958 L32.9077 396.255 L28.3562 396.255 L28.3562 391.958 L18.2347 391.958 L18.2347 386.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M33.8307 345.648 Q33.2578 346.635 33.0032 347.812 Q32.7167 348.958 32.7167 350.359 Q32.7167 355.324 35.9632 357.997 Q39.1779 360.639 45.2253 360.639 L64.0042 360.639 L64.0042 366.527 L28.3562 366.527 L28.3562 360.639 L33.8944 360.639 Q30.6479 358.793 29.0883 355.833 Q27.4968 352.873 27.4968 348.64 Q27.4968 348.035 27.5923 347.303 Q27.656 346.571 27.8151 345.68 L33.8307 345.648 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M32.4621 327.124 Q32.4621 331.834 36.1542 334.572 Q39.8145 337.309 46.212 337.309 Q52.6095 337.309 56.3017 334.603 Q59.9619 331.866 59.9619 327.124 Q59.9619 322.445 56.2698 319.708 Q52.5777 316.97 46.212 316.97 Q39.8781 316.97 36.186 319.708 Q32.4621 322.445 32.4621 327.124 M27.4968 327.124 Q27.4968 319.485 32.4621 315.124 Q37.4273 310.764 46.212 310.764 Q54.9649 310.764 59.9619 315.124 Q64.9272 319.485 64.9272 327.124 Q64.9272 334.794 59.9619 339.155 Q54.9649 343.484 46.212 343.484 Q37.4273 343.484 32.4621 339.155 Q27.4968 334.794 27.4968 327.124 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M58.657 295.391 L77.5631 295.391 L77.5631 301.279 L28.3562 301.279 L28.3562 295.391 L33.7671 295.391 Q30.5842 293.545 29.0564 290.744 Q27.4968 287.911 27.4968 283.996 Q27.4968 277.503 32.6531 273.461 Q37.8093 269.387 46.212 269.387 Q54.6147 269.387 59.771 273.461 Q64.9272 277.503 64.9272 283.996 Q64.9272 287.911 63.3994 290.744 Q61.8398 293.545 58.657 295.391 M46.212 275.466 Q39.7508 275.466 36.0905 278.14 Q32.3984 280.781 32.3984 285.428 Q32.3984 290.075 36.0905 292.749 Q39.7508 295.391 46.212 295.391 Q52.6732 295.391 56.3653 292.749 Q60.0256 290.075 60.0256 285.428 Q60.0256 280.781 56.3653 278.14 Q52.6732 275.466 46.212 275.466 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M67.3143 244.847 Q73.68 247.33 75.6216 249.685 Q77.5631 252.04 77.5631 255.987 L77.5631 260.666 L72.6615 260.666 L72.6615 257.228 Q72.6615 254.809 71.5157 253.473 Q70.3699 252.136 66.1048 250.512 L63.4312 249.462 L28.3562 263.88 L28.3562 257.674 L56.238 246.534 L28.3562 235.394 L28.3562 229.187 L67.3143 244.847 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip852)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,85.838 543.47,95.623 697.455,106.7 806.709,147.238 891.453,221.552 960.694,313.335 1019.24,388.989 1114.68,497.963 1190.89,558.743 1254.33,640.349 \n",
       "  1308.68,700.38 1398.45,788.87 1454.13,832.155 1531.9,871.751 1596.43,938.338 1671.55,1003.19 1734.25,1031.08 1808.95,1099.14 1877.06,1140.28 1944.41,1185.8 \n",
       "  2013.65,1227.94 2085.52,1262.14 2154.21,1311.41 2223.91,1350.59 2292.39,1372.3 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip850)\" d=\"\n",
       "M1992.24 196.379 L2281.66 196.379 L2281.66 92.6992 L1992.24 92.6992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1992.24,196.379 2281.66,196.379 2281.66,92.6992 1992.24,92.6992 1992.24,196.379 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip850)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2015.94,144.539 2158.13,144.539 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip850)\" d=\"M2195.67 164.227 Q2193.87 168.856 2192.16 170.268 Q2190.44 171.68 2187.57 171.68 L2184.17 171.68 L2184.17 168.115 L2186.67 168.115 Q2188.43 168.115 2189.4 167.282 Q2190.37 166.449 2191.55 163.347 L2192.32 161.403 L2181.83 135.893 L2186.35 135.893 L2194.45 156.171 L2202.55 135.893 L2207.06 135.893 L2195.67 164.227 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip850)\" d=\"M2214.35 157.884 L2221.99 157.884 L2221.99 131.518 L2213.68 133.185 L2213.68 128.926 L2221.95 127.259 L2226.62 127.259 L2226.62 157.884 L2234.26 157.884 L2234.26 161.819 L2214.35 161.819 L2214.35 157.884 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "plot(curva_aprendizado.parameter_values,\n",
    "     curva_aprendizado.measurements,\n",
    "     xlab = curva_aprendizado.parameter_name,\n",
    "     xscale = curva_aprendizado.parameter_scale,\n",
    "     ylab = \"Erro Baseado na Cross Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo em disco\n",
    "MLJ.save(\"modelo/modelo_rede_neural.jlso\", mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n",
       "  model: MLJFlux.NeuralNetworkClassifier{MLJFlux.Short, typeof(NNlib.softmax), Flux.Optimise.ADAM, typeof(Flux.Losses.crossentropy)}\n",
       "  args: \n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o modelo do disco\n",
    "mach2 = machine(\"modelo/modelo_rede_neural.jlso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.171, Iris-versicolor=>0.463, Iris-virginica=>0.367)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.124, Iris-versicolor=>0.438, Iris-virginica=>0.438)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.95, Iris-versicolor=>0.048, Iris-virginica=>0.00202)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.108, Iris-versicolor=>0.421, Iris-virginica=>0.471)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.104, Iris-versicolor=>0.419, Iris-virginica=>0.477)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.921, Iris-versicolor=>0.0756, Iris-virginica=>0.00381)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.14, Iris-versicolor=>0.444, Iris-virginica=>0.417)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.154, Iris-versicolor=>0.45, Iris-virginica=>0.395)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.117, Iris-versicolor=>0.425, Iris-virginica=>0.457)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.19, Iris-versicolor=>0.468, Iris-virginica=>0.342)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.117, Iris-versicolor=>0.431, Iris-virginica=>0.452)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.149, Iris-versicolor=>0.452, Iris-virginica=>0.399)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.124, Iris-versicolor=>0.436, Iris-virginica=>0.44)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.121, Iris-versicolor=>0.434, Iris-virginica=>0.445)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.953, Iris-versicolor=>0.0454, Iris-virginica=>0.0018)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.111, Iris-versicolor=>0.425, Iris-virginica=>0.464)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.106, Iris-versicolor=>0.42, Iris-virginica=>0.474)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.11, Iris-versicolor=>0.426, Iris-virginica=>0.464)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.186, Iris-versicolor=>0.475, Iris-virginica=>0.339)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.108, Iris-versicolor=>0.421, Iris-virginica=>0.47)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.96, Iris-versicolor=>0.039, Iris-virginica=>0.00139)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.105, Iris-versicolor=>0.419, Iris-virginica=>0.476)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.112, Iris-versicolor=>0.426, Iris-virginica=>0.462)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.964, Iris-versicolor=>0.0346, Iris-virginica=>0.00121)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.964, Iris-versicolor=>0.0346, Iris-virginica=>0.00119)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faz previsões com dados de teste\n",
    "yhat = predict(mach2, X[test,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.171, Iris-versicolor=>0.463, Iris-virginica=>0.367)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.124, Iris-versicolor=>0.438, Iris-virginica=>0.438)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.95, Iris-versicolor=>0.048, Iris-virginica=>0.00202)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.108, Iris-versicolor=>0.421, Iris-virginica=>0.471)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.104, Iris-versicolor=>0.419, Iris-virginica=>0.477)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza algumas previsões\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36655384234080374"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para obter a probabilidade de \"Iris-virginica\" na primeira previsão\n",
    "pdf(yhat[1], \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45×3 Matrix{Float64}:\n",
       " 0.170753  0.462693   0.366554\n",
       " 0.124039  0.437699   0.438262\n",
       " 0.949999  0.0479837  0.0020171\n",
       " 0.108175  0.421245   0.47058\n",
       " 0.104094  0.418536   0.47737\n",
       " 0.920545  0.0756485  0.003806\n",
       " 0.139528  0.44396    0.416512\n",
       " 0.154455  0.45017    0.395374\n",
       " 0.117336  0.425422   0.457241\n",
       " 0.189829  0.467817   0.342353\n",
       " 0.116904  0.43096    0.452136\n",
       " 0.148718  0.452264   0.399018\n",
       " 0.123777  0.435899   0.440325\n",
       " ⋮                    \n",
       " 0.12117   0.434135   0.444694\n",
       " 0.952847  0.0453568  0.00179613\n",
       " 0.110803  0.424728   0.464469\n",
       " 0.105723  0.420296   0.473981\n",
       " 0.110471  0.425676   0.463853\n",
       " 0.186123  0.474817   0.33906\n",
       " 0.10823   0.421482   0.470289\n",
       " 0.959656  0.0389554  0.00138848\n",
       " 0.104888  0.419022   0.47609\n",
       " 0.111784  0.426243   0.461974\n",
       " 0.964141  0.034645   0.00121363\n",
       " 0.964246  0.0345598  0.00119389"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtém as previsões de cada linha com as probabilidades para cada uma das 3 classes\n",
    "L = levels(y)\n",
    "pdf(yhat, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}

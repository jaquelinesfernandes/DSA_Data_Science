{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q langchain\n",
    "#pip install -q pymupdf\n",
    "#pip install -q huggingface-hub\n",
    "#pip install -q faiss-cpu\n",
    "#pip install -q sentence-transformers\n",
    "#pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "import langchain\n",
    "import textwrap\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import OpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para carregar o pdf\n",
    "def carrega_pdf(file_path):\n",
    "    #Cria uma instancia da classe PyMuPDFLoader passando o caminho\n",
    "    loader = PyMuPDFLoader(file_path=file_path)\n",
    "    \n",
    "    #Usa o metodo 'load' do objeto 'loader' para carregar o conteúdo do PDF\n",
    "    #Isso retorna um objeto ou uma estrutura de dados contendo as paginas do PDF com o seu conteúdo.\n",
    "    docs = loader.load()\n",
    "    \n",
    "    #Retorna o conteúd carregado do PDF\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para dividir os documentos em vários pedações (chuncks)\n",
    "def split_docs(documents, chunck_size = 1000, chunk_overlap = 20):\n",
    "    #Cria uma instância da classe RecursiveCharacterTextSplitter\n",
    "    #Esta classa divide textos longo em pedações menores (chunks)\n",
    "    #'chunk_size' define o tamanho de cada pedaço e 'chunck_overlap' define a sobreposição entre pedações consecutivos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunck_size, chunk_overlap = chunk_overlap, length_function=len, is_separator_regex=False)\n",
    "    \n",
    "    #Utiliza o método 'split_documents' do objeto 'text_spliter' para dividir o documento fornecido.\n",
    "    #'documents' é uma variável que contém o texto ou conjunto de textos a serem divididos\n",
    "    chunks = text_splitter.split_documents(documents = documents)\n",
    "    \n",
    "    #Retorna os pedaçõs de texto resultantes da divisão\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega o modelo de embeddings\n",
    "def carrega_embedding_model(model_path, normalize_embedding=True):\n",
    "    #Retorna uma instância da classe HuggingFaceEmbeddings\n",
    "    # 'model_name' é o identificador do modelo de embeddings a ser carregado\n",
    "    # 'model_kwargs' é um dicionário de argumentos adicionais para a configuração do modelo, neste caso, definindo o dispositivo para 'cpu'\n",
    "    # 'encode_kwargs' é um dicionário de argumentos para o método de codificação, aqui especificando se os embeddings devem ser normalizados.\n",
    "    \n",
    "    return HuggingFaceEmbeddings(model_name=model_path, model_kwargs= {'device': 'cpu'}, encode_kwargs={'normalize_embedding': normalize_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para criar embeddings\n",
    "def cria_embeddings(chunks, embedding_model, storing_path = \"modelo/vectorstore\"):\n",
    "    #Cria um 'vectorstore' (um índice FAISS) a partir dos documentos fornecidos\n",
    "    #'chunks' é a lista de segmentos de texto e 'embedding_model' é o modelo de embedding utilizado para converter texto em embeddings.\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    #Salva o 'vectorstore' criado em um caminho local especificado por 'storing_path'\n",
    "    #Isso permite a persistência do índice FAISS para uso futuro.\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    #Retorna o 'vectorstore' criado, que contém os embeddings e pode ser usado para operações de busca e comparação de similaridade\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a chain\n",
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    #Retorna uma instancia da classe RetrievalQA\n",
    "    #Esta função lida com a cadeia de processo envolvidos em um sistema de Question Answering(QA)\n",
    "    #'llm' refere-se ao modelo de linguagem de grande escala (como um modelo GPT ou BERT)\n",
    "    #'retriever' é um componente usado para recuperar informações relevantes (como um mecanismo de busca ou um retriever de documentos)\n",
    "    #'chain_type' define o tipo de cadeia ou estratégia usada no processo de QA. Aqui, está definido como \"stuff\", um placeholder para um tipo real.\n",
    "    #'return_source_documents': um booleano que, quando True, indica que os documentos fonte (ou seja, os documentos de onde as respostas são extraídas) devem ser retornados juntamente com as respostas\n",
    "    # 'chain_type_kwargs: é um dicionário de argumento adicionais específicos para o tipo de cadeia escolhido. Aqui está passando 'prompt' como argumento\n",
    "    return RetrievalQA.from_chain_type(llm= llm,\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para obter as respostas do LLM (Large Language Model)\n",
    "def get_response(query, chain):\n",
    "    #Invoca a 'chain' (cadeia de processamento, um pipeline de Question Aswering) com 'query' fornecida\n",
    "    #'chain' é uma função que recebe uma consulta e retorna uma resposta, utilizando o LLM\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    #Utiliza a biblioteca textwrap para formatar a resposta. 'textwrap.fill' quebra o texto da resposta em linhas de largura especificada (100 caracteres neste caso),\n",
    "    #tornando mais fácil a leitura em ambientes como o Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    \n",
    "    #Imprime o texto formatado\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a API da Open AI\n",
    "llm_api = OpenAI(openai_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o Modelo Embedding\n",
    "embed = carrega_embedding_model(model_path = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o arquivo PDF\n",
    "#docs = carrega_pdf(file_path = \"../projeto_especial_ia_sql/processos_ds.pdf\")\n",
    "#docs = carrega_pdf(file_path = \"../projeto_especial_ia_sql/processos_metodos_ds.pdf\")\n",
    "docs = carrega_pdf(file_path = \"../projeto_especial_ia_sql/metodos_ds.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Divide o arquivo em Chunks\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m documents \u001b[38;5;241m=\u001b[39m split_docs(documents \u001b[38;5;241m=\u001b[39m docs)\n",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m, in \u001b[0;36msplit_docs\u001b[1;34m(documents, chunck_size, chunk_overlap)\u001b[0m\n\u001b[0;32m      6\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunck_size, chunk_overlap \u001b[38;5;241m=\u001b[39m chunk_overlap, length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m, is_separator_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Utiliza o método 'split_documents' do objeto 'text_spliter' para dividir o documento fornecido.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#'documents' é uma variável que contém o texto ou conjunto de textos a serem divididos\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents \u001b[38;5;241m=\u001b[39m documents)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Retorna os pedaçõs de texto resultantes da divisão\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\langchain\\text_splitter.py:155\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    153\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[0;32m    154\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\langchain\\text_splitter.py:140\u001b[0m, in \u001b[0;36mTextSplitter.create_documents\u001b[1;34m(self, texts, metadatas)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[0;32m    139\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text(text):\n\u001b[0;32m    141\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(_metadatas[i])\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_start_index:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\langchain\\text_splitter.py:693\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_text(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separators)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\langchain\\text_splitter.py:656\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[1;34m(self, text, separators)\u001b[0m\n\u001b[0;32m    654\u001b[0m final_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# Get appropriate separator to use\u001b[39;00m\n\u001b[1;32m--> 656\u001b[0m separator \u001b[38;5;241m=\u001b[39m separators[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    657\u001b[0m new_separators \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(separators):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Divide o arquivo em Chunks\n",
    "documents = split_docs(documents = docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria vectorstore\n",
    "vectorstore = cria_embeddings(documents, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o vectorstore em um retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria template\n",
    "template = \"\"\"\n",
    "### System:\n",
    "Você é um assistente pessoal. Você tem que responder as perguntas do usuário \\\n",
    "usando apenas o contexto fornecido a você. Se você não sabe a resposta, \\\n",
    "apenas diga que você não sabe. Não tente inventar uma resposta.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando prompt a partir do template\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a chain\n",
    "dsa_chain = load_qa_chain(retriever, llm_api, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interagindo com o assistente\n",
    "get_response(\"Quais são os processos de trabalho de um cientista de dados executa?\", dsa_chain)\n",
    "\n",
    "get_response(\"Quais métodos um cientista de dados utiliza?\", dsa_chain)\n",
    "\n",
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
